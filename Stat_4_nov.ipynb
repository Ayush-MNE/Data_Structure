{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the properties of the F-distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The F-distribution is a continuous probability distribution that arises frequently in statistics, especially in the analysis of variance (ANOVA), regression analysis, and hypothesis testing involving variances. Here are its key properties:\n",
    "\n",
    "1. Definition and Shape\n",
    "The F-distribution is the distribution of the ratio of two independent chi-squared variables divided by their respective degrees of freedom.\n",
    "It is right-skewed, especially when degrees of freedom are small. As degrees of freedom increase, the distribution becomes more symmetric.\n",
    "\n",
    "2. Parameters\n",
    "The F-distribution has two parameters: ùëë1 and ùëë2, which are the degrees of freedom for the numerator and denominator, respectively. These determine the exact shape and spread of the distribution.\n",
    "3. Range and Non-negativity\n",
    "The F-distribution is defined only for non-negative values: F‚â•0.\n",
    "It has a range of [0,+‚àû)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The F-distribution is primarily used in statistical tests that compare variances or assess the overall fit of a model, due to its properties of comparing ratios of variances. Here are some of the main types of tests and why the F-distribution is appropriate for each:\n",
    "\n",
    "1. Analysis of Variance (ANOVA)\n",
    "Purpose: ANOVA is used to test whether there are significant differences between the means of multiple groups.\n",
    "Why F-distribution: In ANOVA, the F-test compares the variance between group means to the variance within groups. The F-distribution is appropriate because it allows for comparing two independent estimates of variance (between-group and within-group) to determine if the group means differ significantly.\n",
    "\n",
    "2. Regression Analysis (Overall Significance)\n",
    "Purpose: In multiple regression, the F-test evaluates whether the regression model as a whole significantly predicts the outcome variable.\n",
    "Why F-distribution: Here, the F-statistic is used to compare the variance explained by the model (due to the predictors) to the residual variance (unexplained variance). The F-distribution is suitable as it tests if the overall fit of the model is better than what would be expected by chance.\n",
    "\n",
    "3. Comparison of Two Variances (Variance Ratio Test)\n",
    "Purpose: The F-test is used to compare the variances of two independent normal populations to see if they are equal.\n",
    "Why F-distribution: By taking the ratio of two independent sample variances, we get an F-statistic that follows an F-distribution if the null hypothesis (equal variances) is true. The right-skewed nature of the F-distribution makes it useful for detecting if one variance is significantly larger than the other.\n",
    "\n",
    "4. Testing Nested Models in Regression (Partial F-test)\n",
    "Purpose: This test evaluates whether additional predictors in a more complex model provide a significantly better fit than a simpler (nested) model.\n",
    "Why F-distribution: The partial F-test compares the explained variance between the full and nested models. The ratio of these variances, which follows an F-distribution under the null hypothesis, helps determine if the additional predictors significantly improve the model.\n",
    "\n",
    "5. Multivariate Analysis of Variance (MANOVA)\n",
    "Purpose: MANOVA extends ANOVA to multiple dependent variables to test if mean vectors are equal across groups.\n",
    "Why F-distribution: MANOVA involves a test of ratios of variances for multiple dependent variables simultaneously, often using Wilks' Lambda or other statistics that are converted into an F-statistic. The F-distribution here helps assess whether the overall multivariate group differences are significant.\n",
    "\n",
    "Why the F-Distribution is Appropriate\n",
    "The F-distribution is appropriate for these tests because:\n",
    "\n",
    "It arises from ratios of variances: Since the F-distribution is derived from the ratio of two independent chi-squared distributions (scaled variances), it is inherently suited to tests comparing variances.\n",
    "Right-Skewed Nature: Many statistical tests (e.g., ANOVA, regression) are interested in whether one variance component is significantly larger than another, which aligns with the F-distribution's right-skewed shape.\n",
    "Degrees of Freedom Flexibility: The F-distribution accommodates varying sample sizes and complexity in models, thanks to its two degrees of freedom parameters, allowing for flexibility across a wide range of tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
    "populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. To conduct an F-test to compare the variances of two populations, several key assumptions must be met to ensure the validity of the test. Here are the main assumptions:\n",
    "\n",
    "1. Normality\n",
    "Each of the two populations should follow a normal distribution. The F-test is sensitive to deviations from normality, and if this assumption is violated, the test results may be unreliable.\n",
    "If the data are not normally distributed, alternative tests like the Levene's test or Brown-Forsythe test, which are less sensitive to non-normality, might be more appropriate.\n",
    "\n",
    "2. Independence\n",
    "The samples from each population must be independent of each other. This means that the selection or measurement of one sample should not influence the selection or measurement of the other.\n",
    "This assumption is crucial because any dependence between samples can bias the estimated variances and distort the F-statistic.\n",
    "\n",
    "3. Random Sampling\n",
    "The samples must be randomly selected from the respective populations to ensure that they are representative. Random sampling minimizes bias and increases the generalizability of the results.\n",
    "\n",
    "4. Measurement Scale\n",
    "The data should be measured on an interval or ratio scale (i.e., continuous data). The F-test is not appropriate for ordinal or nominal data, as it relies on calculating variances, which are meaningful only for interval or ratio scales.\n",
    "\n",
    "5. Positive Variances\n",
    "Both sample variances must be positive, as the F-statistic is a ratio of two variances. Negative or zero variances do not make sense in this context and would make the F-test impossible to compute.\n",
    "Consequences of Violating Assumptions\n",
    "Non-Normality: If the normality assumption is violated, the F-distribution may not accurately model the ratio of variances, which can lead to incorrect conclusions. Transformations of data or non-parametric tests (like the Levene's test) may be considered.\n",
    "Lack of Independence: Violating the independence assumption can severely bias the test, as it inflates the variance estimates, leading to unreliable F-statistics.\n",
    "Non-Random Sampling: Without random sampling, the test results may not be generalizable and could reflect bias in sample selection.\n",
    "These assumptions ensure that the F-test provides a reliable comparison of variances. Meeting these conditions allows the F-test to accurately determine if the two populations have significantly different variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The purpose of Analysis of Variance (ANOVA) is to test for statistically significant differences in means among multiple groups. While a t-test is used to compare the means of two groups, ANOVA is an extension of this approach, allowing for comparisons across three or more groups simultaneously.\n",
    "\n",
    "Here‚Äôs a breakdown of the purpose of ANOVA and how it differs from a t-test:\n",
    "\n",
    "Purpose of ANOVA\n",
    "Identify Differences in Group Means: ANOVA assesses whether there are significant differences in means across multiple groups. It tests the null hypothesis that all group means are equal, against the alternative that at least one group mean is different.\n",
    "Control for Type I Error: When comparing multiple groups, performing multiple t-tests increases the risk of a Type I error (false positive). ANOVA controls for this by testing all groups simultaneously under one test, thus reducing the likelihood of incorrectly concluding that differences exist when they don‚Äôt.\n",
    "Applications in Experimental Design: ANOVA is commonly used in experiments where subjects or samples are divided into multiple treatment groups to determine if different treatments lead to different outcomes.\n",
    "How ANOVA Differs from a t-test\n",
    "Number of Groups:\n",
    "\n",
    "t-test: Typically used to compare the means of two groups (e.g., independent samples t-test or paired samples t-test).\n",
    "ANOVA: Used to compare the means of three or more groups.\n",
    "Test Statistic:\n",
    "\n",
    "t-test: The test statistic (t) is based on the difference between two sample means divided by the standard error of the difference.\n",
    "ANOVA: The test statistic (F) is a ratio of the variance between group means to the variance within groups. A larger F-value suggests that the group means are more spread out relative to the within-group variation, indicating potential differences among the groups.\n",
    "\n",
    "Interpretation of Results:\n",
    "\n",
    "t-test: Provides a direct comparison between two groups, indicating if one group mean is significantly higher or lower than the other.\n",
    "ANOVA: Tells if there is a significant difference among groups overall, but it does not specify which groups differ. To identify specific group differences, post hoc tests (e.g., Tukey's HSD) are required following a significant ANOVA result.\n",
    "\n",
    "Type I Error Control:\n",
    "\n",
    "t-test: When used repeatedly across multiple group comparisons, the risk of Type I error increases.\n",
    "ANOVA: By handling multiple groups simultaneously, ANOVA reduces the risk of Type I error that would arise from performing multiple pairwise t-tests.\n",
    "In summary, ANOVA is the preferred approach for comparing multiple groups in a single test, providing an overall indication of whether differences exist among groups, while a t-test is ideal for simpler, two-group comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
    "than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. When comparing more than two groups, a one-way ANOVA is typically preferred over multiple t-tests because it provides an efficient and statistically rigorous way to test for differences among group means. Here‚Äôs why and when you would use a one-way ANOVA instead of multiple t-tests:\n",
    "\n",
    "1. Control for Type I Error Rate\n",
    "Why: Each time a t-test is conducted, there is a chance of committing a Type I error (false positive), where you incorrectly conclude that a difference exists. When multiple t-tests are performed on the same dataset, the probability of at least one Type I error increases significantly. For example, with three groups, performing three pairwise t-tests (group 1 vs. group 2, group 1 vs. group 3, and group 2 vs. group 3) inflates the cumulative Type I error rate.\n",
    "How ANOVA Helps: A one-way ANOVA conducts a single test for all groups simultaneously, maintaining the overall Type I error rate (usually at 5%) across all comparisons. By testing for an overall difference, ANOVA avoids the inflated error rate associated with multiple t-tests.\n",
    "2. Efficiency and Simplicity\n",
    "Why: Running multiple t-tests can be cumbersome and inefficient, especially as the number of groups increases. For example, with four groups, you would need to conduct six t-tests, and with five groups, you would need ten t-tests. This quickly becomes impractical as the number of groups grows.\n",
    "How ANOVA Helps: One-way ANOVA simplifies the analysis by using a single test to determine if there are any differences among all groups, streamlining the comparison process.\n",
    "3. Overall Significance\n",
    "Why: Multiple t-tests provide pairwise comparisons but don‚Äôt answer the overall question: \"Are there any significant differences among these groups at all?\" A set of significant pairwise results might suggest differences, but they don't test for a general effect across all groups.\n",
    "How ANOVA Helps: A one-way ANOVA addresses the overall research question by assessing whether there is evidence of a difference in means among all groups. If the ANOVA result is significant, this indicates that at least one group differs from the others, prompting further analysis.\n",
    "4. Usefulness of Post Hoc Tests\n",
    "Why: Multiple t-tests individually show pairwise differences but don‚Äôt account for all group comparisons in a single framework.\n",
    "How ANOVA Helps: When a one-way ANOVA shows a significant result, post hoc tests (e.g., Tukey's HSD) can be conducted to identify specific group differences while controlling for Type I error. This is more robust than using a series of individual t-tests and provides clearer insights into which groups differ significantly.\n",
    "When to Use One-Way ANOVA\n",
    "Use one-way ANOVA when you have three or more independent groups and want to determine if there are statistically significant differences among their means.\n",
    "It is most appropriate when:\n",
    "The data for each group follow a normal distribution.\n",
    "The variances among groups are roughly equal (homogeneity of variances).\n",
    "The samples are independent.\n",
    "In summary, one-way ANOVA is preferred over multiple t-tests when comparing more than two groups because it controls the Type I error rate, is more efficient, provides an overall significance test, and allows for robust post hoc analysis if significant differences are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
    "How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. In ANOVA, variance is partitioned into two main components: between-group variance and within-group variance. This partitioning is essential for understanding the sources of variation in the data and for calculating the F-statistic, which determines if there are significant differences between group means.\n",
    "\n",
    "Partitioning of Variance in ANOVA\n",
    "Between-Group Variance (Variation Due to Treatment)\n",
    "\n",
    "Definition: Between-group variance measures the variation in the data that is attributed to differences between the group means. It reflects how much the group means deviate from the overall mean (the mean of all data combined).\n",
    "Calculation: It is calculated by taking the squared differences between each group mean and the overall mean, then multiplying by the sample size of each group, and finally averaging across all groups. This component captures the systematic effect of the group factor (e.g., different treatments or conditions).\n",
    "Interpretation: A high between-group variance suggests that group means differ considerably from each other, which may indicate that at least one group mean is significantly different from the others.\n",
    "Within-Group Variance (Variation Due to Random Error)\n",
    "\n",
    "Definition: Within-group variance measures the variation within each group, due to individual differences or random error. It reflects the average deviation of individual observations from their respective group mean.\n",
    "Calculation: It is calculated by taking the squared differences between each observation and its group mean, then summing these differences across all groups, and averaging. This component captures the variability that is not explained by the group factor.\n",
    "Interpretation: High within-group variance suggests a large amount of random variability within groups, which can obscure differences between group means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
    "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The classical (frequentist) and Bayesian approaches to ANOVA both aim to evaluate differences among group means, but they differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Here‚Äôs a comparison of the key aspects:\n",
    "\n",
    "1. Handling of Uncertainty\n",
    "Frequentist ANOVA:\n",
    "\n",
    "Treats parameters as fixed but unknown values. Uncertainty is quantified solely through the sample data.\n",
    "Uncertainty is handled through confidence intervals and p-values, which represent the probability of observing the data (or something more extreme) under the null hypothesis.\n",
    "The frequentist approach uses the concept of repeated sampling to define probability: if the study were repeated infinitely, the p-value would represent the proportion of times we would observe a result as extreme as (or more extreme than) the current data, given that the null hypothesis is true.\n",
    "Bayesian ANOVA:\n",
    "\n",
    "Treats parameters as random variables with probability distributions that capture uncertainty about their values.\n",
    "Prior distributions are used to express any initial beliefs or knowledge about the parameters before observing data, and these priors are updated with the observed data to produce posterior distributions.\n",
    "Uncertainty is directly incorporated into the model through these prior and posterior distributions, providing a more intuitive way to express uncertainty about parameters and effect sizes.\n",
    "2. Parameter Estimation\n",
    "Frequentist ANOVA:\n",
    "\n",
    "Uses point estimates (e.g., group means, mean square errors) calculated from the sample data to estimate parameters.\n",
    "Parameters are estimated in a way that minimizes error (e.g., least squares), but these estimates are assumed to be fixed values, with uncertainty quantified through confidence intervals.\n",
    "Hypothesis testing in frequentist ANOVA focuses on null hypothesis significance testing (NHST), where we test if group means are equal without directly estimating the effect size or the probability of the hypotheses being true.\n",
    "Bayesian ANOVA:\n",
    "\n",
    "Estimates parameters through posterior distributions, which combine prior beliefs with the observed data to yield a full distribution of plausible parameter values.\n",
    "Instead of providing a single estimate, Bayesian ANOVA yields a distribution for each parameter (e.g., group mean differences), giving a range of likely values with associated probabilities.\n",
    "This approach provides direct probability statements about parameters, such as the probability that one group mean is greater than another or the probability distribution of the effect size.\n",
    "3. Hypothesis Testing and Interpretation\n",
    "Frequentist ANOVA:\n",
    "\n",
    "Hypothesis testing is done using an F-test to compare between-group and within-group variances. The result is a p-value, which indicates whether the observed differences among groups are statistically significant.\n",
    "A significant p-value (typically \n",
    "ùëù\n",
    "<\n",
    "0.05\n",
    "p<0.05) suggests that at least one group mean is different from the others, but it doesn‚Äôt quantify the probability of the hypothesis itself being true or false.\n",
    "Frequentist ANOVA provides no direct information about the probability of the null or alternative hypothesis being true; it only tells us the likelihood of observing the data under the assumption that the null hypothesis is true.\n",
    "Bayesian ANOVA:\n",
    "\n",
    "Hypothesis testing is framed as comparing the probability of different models or hypotheses directly. Bayesian ANOVA can yield the posterior probability of each hypothesis, allowing statements like ‚Äúthere is a 90% probability that group A‚Äôs mean is greater than group B‚Äôs mean.‚Äù\n",
    "Instead of a single p-value, Bayesian ANOVA provides a Bayes factor or posterior probabilities for competing hypotheses. A Bayes factor quantifies the relative evidence for one model or hypothesis over another.\n",
    "Bayesian interpretation allows for more nuanced conclusions about hypotheses, offering probability statements (e.g., credible intervals) about group differences rather than binary accept/reject outcomes.\n",
    "4. Prior Information and Flexibility\n",
    "Frequentist ANOVA:\n",
    "\n",
    "Does not incorporate prior information about parameters. The analysis is based entirely on the observed data and the assumption of random sampling.\n",
    "This approach is less flexible in incorporating expert knowledge or prior research findings.\n",
    "Bayesian ANOVA:\n",
    "\n",
    "Allows for the inclusion of prior information through the choice of priors, which can reflect previous research, expert judgment, or be non-informative if no prior information is available.\n",
    "This flexibility makes Bayesian ANOVA useful in settings where prior knowledge is relevant or data is limited, as priors can guide the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Question: You have two sets of data representing the incomes of two different professions1\n",
    "V Profession A: [48, 52, 55, 60, 62]\n",
    "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null hypothesis is Va=Vb\n",
    "# alternate hypothesis is Va!=Vb\n",
    "\n",
    "a=[48,52,55,60,62]\n",
    "b=[45,50,55,52,47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "Va=np.var(a)\n",
    "Vb=np.var(b)\n",
    "Fscore=Va/Vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "alpha=0.05\n",
    "dof1=len(a)-1\n",
    "dof2=len(b)-1\n",
    "Fcritical=scipy.stats.f.ppf(alpha,dof1,dof2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we fail null hypothesis means both profession have different varience.\n"
     ]
    }
   ],
   "source": [
    "if Fscore<=Fcritical:\n",
    "    print('we fail to reject null hypothesis means both profession have same variability')\n",
    "\n",
    "else:\n",
    "    print('we fail null hypothesis means both profession have different varience.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
    "average heights between three different regions with the following data1\n",
    "V Region A: [160, 162, 165, 158, 164]\n",
    "V Region B: [172, 175, 170, 168, 174]\n",
    "V Region C: [180, 182, 179, 185, 183]\n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the results.\n",
    "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[160, 162, 165, 158, 164]\n",
    "B= [172, 175, 170, 168, 174]\n",
    "C= [180, 182, 179, 185, 183]\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import statistics\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "dof1=len(A)-1\n",
    "dof2=len(B)-1\n",
    "dof3=len(C)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "annova_stat, P_value= f_oneway(A,B,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.8706641879370266e-07)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha=0.05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we reject null hypothesis means there is significant difference bet mean height of three region\n"
     ]
    }
   ],
   "source": [
    "if P_value<alpha:\n",
    "    print('we reject null hypothesis means there is significant difference bet mean height of three region')\n",
    "\n",
    "else:\n",
    "    print('we fail to reject null hypothesis means there is no significant difference bet mean height of three region')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
